from pyspark.sql import functions as F
from pyspark.sql.window import Window


# üß≠ 1Ô∏è‚É£ –ë–∞–∑–æ–≤–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ –¥–∞—Ç–∞—Å–µ—Ç
def dataset_info(df):
    print("üìä –ö—ñ–ª—å–∫—ñ—Å—Ç—å —Ä—è–¥–∫—ñ–≤:", df.count())
    print("üìã –ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–æ–ª–æ–Ω–æ–∫:", len(df.columns))
    df.printSchema()


# üî¢ 2Ô∏è‚É£ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —á–∏—Å–ª–æ–≤–∏—Ö —Å—Ç–æ–≤–ø—Ü—è—Ö
def numeric_stats(df):
    print("üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —á–∏—Å–ª–æ–≤–∏—Ö –∫–æ–ª–æ–Ω–∫–∞—Ö:")
    df.select("startYear", "runtimeMinutes", "isAdult").describe().show()


# üíº 3Ô∏è‚É£ –ë—ñ–∑–Ω–µ—Å-–∑–∞–ø–∏—Ç–∏
def business_queries(df):
    print("\n=== 1. –§—ñ–ª—å–º—ñ–≤ —É –∫–æ–∂–Ω–æ–º—É –∂–∞–Ω—Ä—ñ (groupBy + count) ===")
    df.groupBy("genres").count().orderBy(F.desc("count")).show(10)

    print("\n=== 2. –ö—ñ–ª—å–∫—ñ—Å—Ç—å —Ñ—ñ–ª—å–º—ñ–≤ –ø–æ —Ä–æ–∫–∞—Ö (groupBy + count) ===")
    df.groupBy("startYear").count().orderBy(F.desc("startYear")).show(10)

    print("\n=== 3. –¢–æ–ø-10 –Ω–∞–π–¥–æ–≤—à–∏—Ö —Ñ—ñ–ª—å–º—ñ–≤ (filter + orderBy) ===")
    df.filter(F.col("runtimeMinutes") > 0) \
      .orderBy(F.desc("runtimeMinutes")) \
      .select("primaryTitle", "runtimeMinutes") \
      .show(10)

    print("\n=== 4. –§—ñ–ª—å–º–∏ –¥–ª—è –¥–æ—Ä–æ—Å–ª–∏—Ö (filter) ===")
    df.filter(F.col("isAdult") == 1).select("primaryTitle", "startYear").show(5)

    print("\n=== 5. –ö–æ—Ä–æ—Ç–∫—ñ —Ñ—ñ–ª—å–º–∏ < 10 —Ö–≤ (filter + count) ===")
    short_count = df.filter(F.col("runtimeMinutes") < 10).count()
    print("–ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ñ—ñ–ª—å–º—ñ–≤:", short_count)

    print("\n=== 6. –°–µ—Ä–µ–¥–Ω—è —Ç—Ä–∏–≤–∞–ª—ñ—Å—Ç—å –∑–∞ –∂–∞–Ω—Ä–∞–º–∏ (groupBy + avg) ===")
    df.groupBy("genres") \
      .agg(F.avg("runtimeMinutes").alias("avg_length")) \
      .orderBy(F.desc("avg_length")) \
      .show(10)


# üîÑ 4Ô∏è‚É£ –ü—Ä–∏–∫–ª–∞–¥ JOIN (—ñ–º—ñ—Ç–∞—Ü—ñ—è –¥–ª—è IMDB)
def join_examples(df):
    # –°—Ç–≤–æ—Ä–∏–º–æ —É–º–æ–≤–Ω–∏–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –∑ —Ä–µ–π—Ç–∏–Ω–≥–∞–º–∏ (—ñ–º—ñ—Ç–∞—Ü—ñ—è —ñ–Ω—à–æ–≥–æ IMDB —Ñ–∞–π–ª—É)
    ratings_data = [
        ("tt0000001", 5.6, 200),
        ("tt0000002", 6.0, 180),
        ("tt0000003", 7.3, 250),
        ("tt0000004", 5.9, 100),
        ("tt0000005", 6.4, 150)
    ]
    ratings_df = df.sparkSession.createDataFrame(ratings_data, ["tconst", "averageRating", "numVotes"])

    print("\n=== 7. Join —ñ–∑ —Ä–µ–π—Ç–∏–Ω–≥–∞–º–∏ (inner join) ===")
    joined = df.join(ratings_df, on="tconst", how="inner")
    joined.select("primaryTitle", "averageRating", "numVotes").show(5)

    print("\n=== 8. Join –¥–ª—è –ø–æ—à—É–∫—É —Ñ—ñ–ª—å–º—ñ–≤ > 6.0 —Ä–µ–π—Ç–∏–Ω–≥—É ===")
    joined.filter(F.col("averageRating") > 6.0).select("primaryTitle", "averageRating").show(5)


# ü™ü 5Ô∏è‚É£ –ü—Ä–∏–∫–ª–∞–¥ WINDOW —Ñ—É–Ω–∫—Ü—ñ–π
def window_examples(df):
    window_spec = Window.partitionBy("genres").orderBy(F.desc("runtimeMinutes"))

    print("\n=== 9. Top-1 –Ω–∞–π–¥–æ–≤—à–∏–π —Ñ—ñ–ª—å–º —É –∫–æ–∂–Ω–æ–º—É –∂–∞–Ω—Ä—ñ (window) ===")
    df.withColumn("rank", F.row_number().over(window_spec)) \
      .filter(F.col("rank") == 1) \
      .select("genres", "primaryTitle", "runtimeMinutes") \
      .orderBy(F.desc("runtimeMinutes")) \
      .show(10)

    print("\n=== 10. –°–µ—Ä–µ–¥–Ω—è —Ç—Ä–∏–≤–∞–ª—ñ—Å—Ç—å –ø–æ –∂–∞–Ω—Ä–∞—Ö (window avg) ===")
    df.withColumn("avg_len_genre", F.avg("runtimeMinutes").over(Window.partitionBy("genres"))) \
      .select("genres", "primaryTitle", "runtimeMinutes", "avg_len_genre") \
      .show(10)


import os

def save_results(df, path="output/results.csv"):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    pdf = df.limit(1000).toPandas()  # –∑–±–µ—Ä–µ–∂–µ–º–æ –ª–∏—à–µ —á–∞—Å—Ç–∏–Ω—É, —â–æ–± –Ω–µ –ø–µ—Ä–µ–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ RAM
    pdf.to_csv(path, index=False, encoding="utf-8-sig")
    print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É {os.path.abspath(path)}")

